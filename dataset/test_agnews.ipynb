{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "14e0fd6e-ad6a-4fc7-86cb-fd4f5398334e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from datasets import load_dataset\n",
    "from transformers import DistilBertTokenizerFast, DistilBertForSequenceClassification, AdamW\n",
    "from tqdm.auto import tqdm\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5ae66b82-afe0-479b-a66a-96797030f768",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Load the AG News dataset\n",
    "agnews_train = load_dataset(\"fancyzhx/ag_news\", split=\"train\")\n",
    "agnews_test = load_dataset(\"fancyzhx/ag_news\", split=\"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9d141da5-12cc-4111-be8e-2ca9ac9d517f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Initialize the tokenizer\n",
    "tokenizer = DistilBertTokenizerFast.from_pretrained('distilbert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3216ec08-a4a5-47e0-9d8b-7ecfc53f1d2e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Tokenize the datasets\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples['text'], padding=\"max_length\", truncation=True, max_length=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a0da924a-16aa-4e07-a3c9-a6fe1f4ae48b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "tokenized_train = agnews_train.map(tokenize_function, batched=True)\n",
    "tokenized_test = agnews_test.map(tokenize_function, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bfd8390d-081d-4ca8-8315-69fb2c658c6b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Convert datasets to PyTorch Dataset\n",
    "class AGNewsDataset(Dataset):\n",
    "    def __init__(self, encodings):\n",
    "        self.encodings = encodings\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.encodings['input_ids'])\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # item = {key: torch.tensor(self.encodings[key][idx]) for key in self.encodings if key != 'label'}\n",
    "        item = {key: torch.tensor(self.encodings[key][idx]) for key in self.encodings if key != 'label'}\n",
    "        item = {key: torch.tensor(self.encodings[key][idx]) for key in self.encodings.features if key != 'label'}\n",
    "        item['labels'] = torch.tensor(self.encodings['label'][idx])\n",
    "        return item"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6189a629-d414-4547-8138-e588ce10a527",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_dataset = AGNewsDataset(tokenized_train)\n",
    "test_dataset = AGNewsDataset(tokenized_test)\n",
    "\n",
    "# Create DataLoader\n",
    "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1db1fb87-e06c-40b9-b847-f6f40e10ab78",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of samples in the training dataset: 120000\n",
      "First sample in the training dataset:\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "invalid literal for int() with base 10: 'text'",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mValueError\u001B[0m                                Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[8], line 6\u001B[0m\n\u001B[1;32m      4\u001B[0m \u001B[38;5;66;03m# Print the first sample\u001B[39;00m\n\u001B[1;32m      5\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mFirst sample in the training dataset:\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m----> 6\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[43mtrain_dataset\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;241;43m0\u001B[39;49m\u001B[43m]\u001B[49m)\n\u001B[1;32m      8\u001B[0m \u001B[38;5;66;03m# Print a few samples\u001B[39;00m\n\u001B[1;32m      9\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m i \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mrange\u001B[39m(\u001B[38;5;241m5\u001B[39m):\n",
      "Cell \u001B[0;32mIn[6], line 11\u001B[0m, in \u001B[0;36mAGNewsDataset.__getitem__\u001B[0;34m(self, idx)\u001B[0m\n\u001B[1;32m      9\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m__getitem__\u001B[39m(\u001B[38;5;28mself\u001B[39m, idx):\n\u001B[1;32m     10\u001B[0m     \u001B[38;5;66;03m# item = {key: torch.tensor(self.encodings[key][idx]) for key in self.encodings if key != 'label'}\u001B[39;00m\n\u001B[0;32m---> 11\u001B[0m     item \u001B[38;5;241m=\u001B[39m {key: torch\u001B[38;5;241m.\u001B[39mtensor(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mencodings[key][idx]) \u001B[38;5;28;01mfor\u001B[39;00m key \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mencodings \u001B[38;5;28;01mif\u001B[39;00m key \u001B[38;5;241m!=\u001B[39m \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mlabel\u001B[39m\u001B[38;5;124m'\u001B[39m}\n\u001B[1;32m     12\u001B[0m     item \u001B[38;5;241m=\u001B[39m {key: torch\u001B[38;5;241m.\u001B[39mtensor(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mencodings[key][idx]) \u001B[38;5;28;01mfor\u001B[39;00m key \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mencodings\u001B[38;5;241m.\u001B[39mfeatures \u001B[38;5;28;01mif\u001B[39;00m key \u001B[38;5;241m!=\u001B[39m \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mlabel\u001B[39m\u001B[38;5;124m'\u001B[39m}\n\u001B[1;32m     13\u001B[0m     item[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mlabels\u001B[39m\u001B[38;5;124m'\u001B[39m] \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39mtensor(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mencodings[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mlabel\u001B[39m\u001B[38;5;124m'\u001B[39m][idx])\n",
      "Cell \u001B[0;32mIn[6], line 11\u001B[0m, in \u001B[0;36m<dictcomp>\u001B[0;34m(.0)\u001B[0m\n\u001B[1;32m      9\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m__getitem__\u001B[39m(\u001B[38;5;28mself\u001B[39m, idx):\n\u001B[1;32m     10\u001B[0m     \u001B[38;5;66;03m# item = {key: torch.tensor(self.encodings[key][idx]) for key in self.encodings if key != 'label'}\u001B[39;00m\n\u001B[0;32m---> 11\u001B[0m     item \u001B[38;5;241m=\u001B[39m {key: torch\u001B[38;5;241m.\u001B[39mtensor(\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mencodings\u001B[49m\u001B[43m[\u001B[49m\u001B[43mkey\u001B[49m\u001B[43m]\u001B[49m[idx]) \u001B[38;5;28;01mfor\u001B[39;00m key \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mencodings \u001B[38;5;28;01mif\u001B[39;00m key \u001B[38;5;241m!=\u001B[39m \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mlabel\u001B[39m\u001B[38;5;124m'\u001B[39m}\n\u001B[1;32m     12\u001B[0m     item \u001B[38;5;241m=\u001B[39m {key: torch\u001B[38;5;241m.\u001B[39mtensor(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mencodings[key][idx]) \u001B[38;5;28;01mfor\u001B[39;00m key \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mencodings\u001B[38;5;241m.\u001B[39mfeatures \u001B[38;5;28;01mif\u001B[39;00m key \u001B[38;5;241m!=\u001B[39m \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mlabel\u001B[39m\u001B[38;5;124m'\u001B[39m}\n\u001B[1;32m     13\u001B[0m     item[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mlabels\u001B[39m\u001B[38;5;124m'\u001B[39m] \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39mtensor(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mencodings[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mlabel\u001B[39m\u001B[38;5;124m'\u001B[39m][idx])\n",
      "File \u001B[0;32m/work/LAS/jannesar-lab/.ondemand/dphuong/jupyter-envs/20230427/lib/python3.10/site-packages/datasets/arrow_dataset.py:2861\u001B[0m, in \u001B[0;36mDataset.__getitem__\u001B[0;34m(self, key)\u001B[0m\n\u001B[1;32m   2859\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m__getitem__\u001B[39m(\u001B[38;5;28mself\u001B[39m, key):  \u001B[38;5;66;03m# noqa: F811\u001B[39;00m\n\u001B[1;32m   2860\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\"Can be used to index columns (by string names) or rows (by integer index or iterable of indices or bools).\"\"\"\u001B[39;00m\n\u001B[0;32m-> 2861\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_getitem\u001B[49m\u001B[43m(\u001B[49m\u001B[43mkey\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m/work/LAS/jannesar-lab/.ondemand/dphuong/jupyter-envs/20230427/lib/python3.10/site-packages/datasets/arrow_dataset.py:2845\u001B[0m, in \u001B[0;36mDataset._getitem\u001B[0;34m(self, key, **kwargs)\u001B[0m\n\u001B[1;32m   2843\u001B[0m format_kwargs \u001B[38;5;241m=\u001B[39m format_kwargs \u001B[38;5;28;01mif\u001B[39;00m format_kwargs \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;28;01melse\u001B[39;00m {}\n\u001B[1;32m   2844\u001B[0m formatter \u001B[38;5;241m=\u001B[39m get_formatter(format_type, features\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_info\u001B[38;5;241m.\u001B[39mfeatures, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mformat_kwargs)\n\u001B[0;32m-> 2845\u001B[0m pa_subtable \u001B[38;5;241m=\u001B[39m \u001B[43mquery_table\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_data\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mkey\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mindices\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_indices\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   2846\u001B[0m formatted_output \u001B[38;5;241m=\u001B[39m format_table(\n\u001B[1;32m   2847\u001B[0m     pa_subtable, key, formatter\u001B[38;5;241m=\u001B[39mformatter, format_columns\u001B[38;5;241m=\u001B[39mformat_columns, output_all_columns\u001B[38;5;241m=\u001B[39moutput_all_columns\n\u001B[1;32m   2848\u001B[0m )\n\u001B[1;32m   2849\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m formatted_output\n",
      "File \u001B[0;32m/work/LAS/jannesar-lab/.ondemand/dphuong/jupyter-envs/20230427/lib/python3.10/site-packages/datasets/formatting/formatting.py:587\u001B[0m, in \u001B[0;36mquery_table\u001B[0;34m(table, key, indices)\u001B[0m\n\u001B[1;32m    585\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m    586\u001B[0m     size \u001B[38;5;241m=\u001B[39m indices\u001B[38;5;241m.\u001B[39mnum_rows \u001B[38;5;28;01mif\u001B[39;00m indices \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;28;01melse\u001B[39;00m table\u001B[38;5;241m.\u001B[39mnum_rows\n\u001B[0;32m--> 587\u001B[0m     \u001B[43m_check_valid_index_key\u001B[49m\u001B[43m(\u001B[49m\u001B[43mkey\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43msize\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    588\u001B[0m \u001B[38;5;66;03m# Query the main table\u001B[39;00m\n\u001B[1;32m    589\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m indices \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n",
      "File \u001B[0;32m/work/LAS/jannesar-lab/.ondemand/dphuong/jupyter-envs/20230427/lib/python3.10/site-packages/datasets/formatting/formatting.py:537\u001B[0m, in \u001B[0;36m_check_valid_index_key\u001B[0;34m(key, size)\u001B[0m\n\u001B[1;32m    535\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(key, Iterable):\n\u001B[1;32m    536\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mlen\u001B[39m(key) \u001B[38;5;241m>\u001B[39m \u001B[38;5;241m0\u001B[39m:\n\u001B[0;32m--> 537\u001B[0m         _check_valid_index_key(\u001B[38;5;28;43mint\u001B[39;49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mmax\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mkey\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m, size\u001B[38;5;241m=\u001B[39msize)\n\u001B[1;32m    538\u001B[0m         _check_valid_index_key(\u001B[38;5;28mint\u001B[39m(\u001B[38;5;28mmin\u001B[39m(key)), size\u001B[38;5;241m=\u001B[39msize)\n\u001B[1;32m    539\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n",
      "\u001B[0;31mValueError\u001B[0m: invalid literal for int() with base 10: 'text'"
     ]
    }
   ],
   "source": [
    "# Check the number of samples in the training dataset\n",
    "print(f\"Number of samples in the training dataset: {len(train_dataset)}\")\n",
    "\n",
    "# Print the first sample\n",
    "print(\"First sample in the training dataset:\")\n",
    "print(train_dataset[0])\n",
    "\n",
    "# Print a few samples\n",
    "for i in range(5):\n",
    "    print(f\"Sample {i}:\")\n",
    "    print(train_dataset[i])\n",
    "\n",
    "# Convert tokenized IDs back to text\n",
    "def decode_example(example):\n",
    "    input_ids = example['input_ids'].tolist()\n",
    "    return tokenizer.decode(input_ids, skip_special_tokens=True)\n",
    "\n",
    "# Print a few decoded samples\n",
    "for i in range(5):\n",
    "    print(f\"Sample {i}:\")\n",
    "    decoded_text = decode_example(train_dataset[i])\n",
    "    print(f\"Text: {decoded_text}\")\n",
    "    print(f\"Label: {train_dataset[i]['labels'].item()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d027047-480e-4690-a5e3-1f2070a339bc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "95ed93ea-a57e-4259-86df-9afb46283e87",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DistilBertForSequenceClassification(\n",
       "  (distilbert): DistilBertModel(\n",
       "    (embeddings): Embeddings(\n",
       "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (transformer): Transformer(\n",
       "      (layer): ModuleList(\n",
       "        (0-5): 6 x TransformerBlock(\n",
       "          (attention): MultiHeadSelfAttention(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (ffn): FFN(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (activation): GELUActivation()\n",
       "          )\n",
       "          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (pre_classifier): Linear(in_features=768, out_features=768, bias=True)\n",
       "  (classifier): Linear(in_features=768, out_features=4, bias=True)\n",
       "  (dropout): Dropout(p=0.2, inplace=False)\n",
       ")"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Initialize the model\n",
    "model = DistilBertForSequenceClassification.from_pretrained('distilbert-base-uncased', num_labels=4)\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "917e5cdf-116c-42a0-8daa-3622a8d708f2",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/work/LAS/jannesar-lab/.ondemand/dphuong/jupyter-envs/20230427/lib/python3.10/site-packages/transformers/optimization.py:588: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Initialize the optimizer\n",
    "optimizer = AdamW(model.parameters(), lr=2e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cd93c4cb-a72b-4962-a0b6-ee24a097e92c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ea5e233a4df94eb587629b4539ce8399",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/7500 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "TypeError",
     "evalue": "new(): invalid data type 'str'",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mTypeError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[10], line 5\u001B[0m\n\u001B[1;32m      3\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m epoch \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mrange\u001B[39m(\u001B[38;5;241m3\u001B[39m):  \u001B[38;5;66;03m# Number of epochs\u001B[39;00m\n\u001B[1;32m      4\u001B[0m     loop \u001B[38;5;241m=\u001B[39m tqdm(train_loader, leave\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m)\n\u001B[0;32m----> 5\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m batch \u001B[38;5;129;01min\u001B[39;00m loop:\n\u001B[1;32m      6\u001B[0m         optimizer\u001B[38;5;241m.\u001B[39mzero_grad()\n\u001B[1;32m      7\u001B[0m         input_ids \u001B[38;5;241m=\u001B[39m batch[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124minput_ids\u001B[39m\u001B[38;5;124m'\u001B[39m]\u001B[38;5;241m.\u001B[39mto(device)\n",
      "File \u001B[0;32m/opt/python310/lib/python3.10/site-packages/tqdm/notebook.py:254\u001B[0m, in \u001B[0;36mtqdm_notebook.__iter__\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m    252\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m    253\u001B[0m     it \u001B[38;5;241m=\u001B[39m \u001B[38;5;28msuper\u001B[39m(tqdm_notebook, \u001B[38;5;28mself\u001B[39m)\u001B[38;5;241m.\u001B[39m\u001B[38;5;21m__iter__\u001B[39m()\n\u001B[0;32m--> 254\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m obj \u001B[38;5;129;01min\u001B[39;00m it:\n\u001B[1;32m    255\u001B[0m         \u001B[38;5;66;03m# return super(tqdm...) will not catch exception\u001B[39;00m\n\u001B[1;32m    256\u001B[0m         \u001B[38;5;28;01myield\u001B[39;00m obj\n\u001B[1;32m    257\u001B[0m \u001B[38;5;66;03m# NB: except ... [ as ...] breaks IPython async KeyboardInterrupt\u001B[39;00m\n",
      "File \u001B[0;32m/opt/python310/lib/python3.10/site-packages/tqdm/std.py:1178\u001B[0m, in \u001B[0;36mtqdm.__iter__\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m   1175\u001B[0m time \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_time\n\u001B[1;32m   1177\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m-> 1178\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m obj \u001B[38;5;129;01min\u001B[39;00m iterable:\n\u001B[1;32m   1179\u001B[0m         \u001B[38;5;28;01myield\u001B[39;00m obj\n\u001B[1;32m   1180\u001B[0m         \u001B[38;5;66;03m# Update and possibly print the progressbar.\u001B[39;00m\n\u001B[1;32m   1181\u001B[0m         \u001B[38;5;66;03m# Note: does not call self.update(1) for speed optimisation.\u001B[39;00m\n",
      "File \u001B[0;32m/opt/python310/lib/python3.10/site-packages/torch/utils/data/dataloader.py:634\u001B[0m, in \u001B[0;36m_BaseDataLoaderIter.__next__\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m    631\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_sampler_iter \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m    632\u001B[0m     \u001B[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001B[39;00m\n\u001B[1;32m    633\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_reset()  \u001B[38;5;66;03m# type: ignore[call-arg]\u001B[39;00m\n\u001B[0;32m--> 634\u001B[0m data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_next_data\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    635\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_num_yielded \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;241m1\u001B[39m\n\u001B[1;32m    636\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_dataset_kind \u001B[38;5;241m==\u001B[39m _DatasetKind\u001B[38;5;241m.\u001B[39mIterable \u001B[38;5;129;01mand\u001B[39;00m \\\n\u001B[1;32m    637\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_IterableDataset_len_called \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;129;01mand\u001B[39;00m \\\n\u001B[1;32m    638\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_num_yielded \u001B[38;5;241m>\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_IterableDataset_len_called:\n",
      "File \u001B[0;32m/opt/python310/lib/python3.10/site-packages/torch/utils/data/dataloader.py:678\u001B[0m, in \u001B[0;36m_SingleProcessDataLoaderIter._next_data\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m    676\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m_next_data\u001B[39m(\u001B[38;5;28mself\u001B[39m):\n\u001B[1;32m    677\u001B[0m     index \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_next_index()  \u001B[38;5;66;03m# may raise StopIteration\u001B[39;00m\n\u001B[0;32m--> 678\u001B[0m     data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_dataset_fetcher\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfetch\u001B[49m\u001B[43m(\u001B[49m\u001B[43mindex\u001B[49m\u001B[43m)\u001B[49m  \u001B[38;5;66;03m# may raise StopIteration\u001B[39;00m\n\u001B[1;32m    679\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_pin_memory:\n\u001B[1;32m    680\u001B[0m         data \u001B[38;5;241m=\u001B[39m _utils\u001B[38;5;241m.\u001B[39mpin_memory\u001B[38;5;241m.\u001B[39mpin_memory(data, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_pin_memory_device)\n",
      "File \u001B[0;32m/opt/python310/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py:51\u001B[0m, in \u001B[0;36m_MapDatasetFetcher.fetch\u001B[0;34m(self, possibly_batched_index)\u001B[0m\n\u001B[1;32m     49\u001B[0m         data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdataset\u001B[38;5;241m.\u001B[39m__getitems__(possibly_batched_index)\n\u001B[1;32m     50\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m---> 51\u001B[0m         data \u001B[38;5;241m=\u001B[39m [\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdataset[idx] \u001B[38;5;28;01mfor\u001B[39;00m idx \u001B[38;5;129;01min\u001B[39;00m possibly_batched_index]\n\u001B[1;32m     52\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m     53\u001B[0m     data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdataset[possibly_batched_index]\n",
      "File \u001B[0;32m/opt/python310/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py:51\u001B[0m, in \u001B[0;36m<listcomp>\u001B[0;34m(.0)\u001B[0m\n\u001B[1;32m     49\u001B[0m         data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdataset\u001B[38;5;241m.\u001B[39m__getitems__(possibly_batched_index)\n\u001B[1;32m     50\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m---> 51\u001B[0m         data \u001B[38;5;241m=\u001B[39m [\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdataset\u001B[49m\u001B[43m[\u001B[49m\u001B[43midx\u001B[49m\u001B[43m]\u001B[49m \u001B[38;5;28;01mfor\u001B[39;00m idx \u001B[38;5;129;01min\u001B[39;00m possibly_batched_index]\n\u001B[1;32m     52\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m     53\u001B[0m     data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdataset[possibly_batched_index]\n",
      "Cell \u001B[0;32mIn[6], line 18\u001B[0m, in \u001B[0;36mAGNewsDataset.__getitem__\u001B[0;34m(self, idx)\u001B[0m\n\u001B[1;32m     14\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m__getitem__\u001B[39m(\u001B[38;5;28mself\u001B[39m, idx):\n\u001B[1;32m     15\u001B[0m     \u001B[38;5;66;03m# item = {key: torch.tensor(self.encodings[key][idx]) for key in self.encodings.features}\u001B[39;00m\n\u001B[1;32m     16\u001B[0m     \u001B[38;5;66;03m# item = {key: torch.tensor(self.encodings[key][idx]) for key in self.encodings}\u001B[39;00m\n\u001B[1;32m     17\u001B[0m     \u001B[38;5;66;03m# item = {key: torch.tensor(self.encodings[key][idx]) for key in self.encodings.keys() if key != 'label'}\u001B[39;00m\n\u001B[0;32m---> 18\u001B[0m     item \u001B[38;5;241m=\u001B[39m {key: torch\u001B[38;5;241m.\u001B[39mtensor(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mencodings[key][idx]) \u001B[38;5;28;01mfor\u001B[39;00m key \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mencodings\u001B[38;5;241m.\u001B[39mcolumn_names \u001B[38;5;28;01mif\u001B[39;00m key \u001B[38;5;241m!=\u001B[39m \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mlabel\u001B[39m\u001B[38;5;124m'\u001B[39m}\n\u001B[1;32m     19\u001B[0m     item[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mlabels\u001B[39m\u001B[38;5;124m'\u001B[39m] \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39mtensor(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mlabels[idx])\n\u001B[1;32m     20\u001B[0m     \u001B[38;5;66;03m# item['labels'] = torch.tensor(self.encodings['label'][idx])\u001B[39;00m\n",
      "Cell \u001B[0;32mIn[6], line 18\u001B[0m, in \u001B[0;36m<dictcomp>\u001B[0;34m(.0)\u001B[0m\n\u001B[1;32m     14\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m__getitem__\u001B[39m(\u001B[38;5;28mself\u001B[39m, idx):\n\u001B[1;32m     15\u001B[0m     \u001B[38;5;66;03m# item = {key: torch.tensor(self.encodings[key][idx]) for key in self.encodings.features}\u001B[39;00m\n\u001B[1;32m     16\u001B[0m     \u001B[38;5;66;03m# item = {key: torch.tensor(self.encodings[key][idx]) for key in self.encodings}\u001B[39;00m\n\u001B[1;32m     17\u001B[0m     \u001B[38;5;66;03m# item = {key: torch.tensor(self.encodings[key][idx]) for key in self.encodings.keys() if key != 'label'}\u001B[39;00m\n\u001B[0;32m---> 18\u001B[0m     item \u001B[38;5;241m=\u001B[39m {key: \u001B[43mtorch\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtensor\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mencodings\u001B[49m\u001B[43m[\u001B[49m\u001B[43mkey\u001B[49m\u001B[43m]\u001B[49m\u001B[43m[\u001B[49m\u001B[43midx\u001B[49m\u001B[43m]\u001B[49m\u001B[43m)\u001B[49m \u001B[38;5;28;01mfor\u001B[39;00m key \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mencodings\u001B[38;5;241m.\u001B[39mcolumn_names \u001B[38;5;28;01mif\u001B[39;00m key \u001B[38;5;241m!=\u001B[39m \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mlabel\u001B[39m\u001B[38;5;124m'\u001B[39m}\n\u001B[1;32m     19\u001B[0m     item[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mlabels\u001B[39m\u001B[38;5;124m'\u001B[39m] \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39mtensor(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mlabels[idx])\n\u001B[1;32m     20\u001B[0m     \u001B[38;5;66;03m# item['labels'] = torch.tensor(self.encodings['label'][idx])\u001B[39;00m\n",
      "\u001B[0;31mTypeError\u001B[0m: new(): invalid data type 'str'"
     ]
    }
   ],
   "source": [
    "# Training loop\n",
    "model.train()\n",
    "for epoch in range(3):  # Number of epochs\n",
    "    loop = tqdm(train_loader, leave=True)\n",
    "    for batch in loop:\n",
    "        optimizer.zero_grad()\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['labels'].to(device)\n",
    "        \n",
    "        outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
    "        \n",
    "        loss = outputs.loss\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        loop.set_description(f'Epoch {epoch}')\n",
    "        loop.set_postfix(loss=loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bce9c60-3d04-4ca0-b3c4-084fe3beabf4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Evaluation loop\n",
    "model.eval()\n",
    "all_preds = []\n",
    "all_labels = []\n",
    "for batch in tqdm(test_loader):\n",
    "    input_ids = batch['input_ids'].to(device)\n",
    "    attention_mask = batch['attention_mask'].to(device)\n",
    "    labels = batch['labels'].to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model(input_ids, attention_mask=attention_mask)\n",
    "        logits = outputs.logits\n",
    "        predictions = torch.argmax(logits, dim=-1)\n",
    "        all_preds.extend(predictions.cpu().numpy())\n",
    "        all_labels.extend(labels.cpu().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fe0da06-395e-4fc9-b677-8ce7982ae8c3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "accuracy = accuracy_score(all_labels, all_preds)\n",
    "print(f\"Accuracy: {accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5ec4693-e63e-4db8-9599-a8a08e91d457",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
