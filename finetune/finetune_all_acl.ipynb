{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "daf777a5-31ae-458c-9fd1-7aa5a4deb91d",
   "metadata": {},
   "source": [
    "# Baseline: Finetune All Layers (Without LoRA)"
   ]
  },
  {
   "cell_type": "code",
   "id": "ffacbc1f-9be9-41c9-bfc6-c3f3c06cec4e",
   "metadata": {
    "tags": [],
    "ExecuteTime": {
     "end_time": "2024-06-10T15:24:20.984226Z",
     "start_time": "2024-06-10T15:24:18.164845Z"
    }
   },
   "source": [
    "%load_ext watermark\n",
    "%watermark --conda -p torch,transformers,datasets,lightning"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch       : 2.3.1\n",
      "transformers: 4.41.2\n",
      "datasets    : 2.19.2\n",
      "lightning   : 2.2.5\n",
      "\n",
      "conda environment: flora_mac\n",
      "\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "cell_type": "code",
   "id": "2c805999-b045-4dc8-b7a4-e166b66013d8",
   "metadata": {
    "tags": [],
    "ExecuteTime": {
     "end_time": "2024-06-10T15:24:24.299676Z",
     "start_time": "2024-06-10T15:24:24.296587Z"
    }
   },
   "source": [
    "import os\n",
    "\n",
    "# Set the SLURM environment variables\n",
    "os.environ[\"SLURM_NTASKS_PER_NODE\"] = \"2\"\n",
    "\n",
    "# Verify that the environment variables are set\n",
    "print(\"SLURM_NTASKS_PER_NODE:\", os.environ.get(\"SLURM_NTASKS_PER_NODE\"))"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SLURM_NTASKS_PER_NODE: 2\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "cell_type": "markdown",
   "id": "96a648c8-8a15-4faf-baf2-564141bd11f8",
   "metadata": {},
   "source": [
    "# 1 Loading the dataset into DataFrames"
   ]
  },
  {
   "cell_type": "code",
   "id": "47994db7-3055-45b1-839c-631d0c5b4819",
   "metadata": {
    "tags": [],
    "ExecuteTime": {
     "end_time": "2024-06-10T15:24:33.693739Z",
     "start_time": "2024-06-10T15:24:33.665056Z"
    }
   },
   "source": [
    "import os\n",
    "from datasets import load_dataset\n",
    "\n",
    "import lightning as L\n",
    "from lightning.pytorch.loggers import CSVLogger\n",
    "from lightning.pytorch.callbacks import ModelCheckpoint\n",
    "\n",
    "import pandas as pd\n",
    "import torch\n",
    "\n",
    "from local_dataset_utilities import download_dataset, load_dataset_into_to_dataframe, partition_dataset\n",
    "from local_dataset_utilities import IMDBDataset"
   ],
   "outputs": [],
   "execution_count": 3
  },
  {
   "cell_type": "code",
   "id": "7ad5d159-feaa-4617-b40a-62455c0e3c98",
   "metadata": {
    "tags": [],
    "ExecuteTime": {
     "end_time": "2024-06-10T15:24:33.886385Z",
     "start_time": "2024-06-10T15:24:33.884165Z"
    }
   },
   "source": [
    "if not torch.cuda.is_available():\n",
    "    print(\"Please switch to a GPU machine before running this notebook.\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Please switch to a GPU machine before running this notebook.\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "888b0250-4198-461c-890a-5f676fa430e4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "files = (\"test.csv\", \"train.csv\", \"val.csv\")\n",
    "download = True\n",
    "\n",
    "for f in files:\n",
    "    if not os.path.exists(os.path.join(\"data\", f)):\n",
    "        download = False\n",
    "\n",
    "if download is False:\n",
    "    download_dataset()\n",
    "    df = load_dataset_into_to_dataframe()\n",
    "    partition_dataset(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0b251b4f-ec6b-4995-afe0-29c4596431fc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_train = pd.read_csv(os.path.join(\"data\", \"train.csv\"))\n",
    "df_val = pd.read_csv(os.path.join(\"data\", \"val.csv\"))\n",
    "df_test = pd.read_csv(os.path.join(\"data\", \"test.csv\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "444fd9ba-5579-46a8-b374-3ab411a575c2",
   "metadata": {},
   "source": [
    "# 2 Tokenization and Numericalization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf6afdd8-cac3-4203-9d00-1e073a6645f5",
   "metadata": {},
   "source": [
    "**Load the dataset via `load_dataset`**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "12895d70-f7f3-420c-b06a-3163997928b0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['index', 'text', 'label'],\n",
      "        num_rows: 35000\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['index', 'text', 'label'],\n",
      "        num_rows: 5000\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['index', 'text', 'label'],\n",
      "        num_rows: 10000\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "imdb_dataset = load_dataset(\n",
    "    \"csv\",\n",
    "    data_files={\n",
    "        \"train\": os.path.join(\"data\", \"train.csv\"),\n",
    "        \"validation\": os.path.join(\"data\", \"val.csv\"),\n",
    "        \"test\": os.path.join(\"data\", \"test.csv\"),\n",
    "    },\n",
    ")\n",
    "\n",
    "print(imdb_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fbf87cf-6134-4a11-bb69-1fb9c5f73f7e",
   "metadata": {
    "tags": []
   },
   "source": [
    "**Tokenize the dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e1dc62df-faa2-49a1-9caa-65905c74b310",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizer input max length: 512\n",
      "Tokenizer vocabulary size: 30522\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
    "print(\"Tokenizer input max length:\", tokenizer.model_max_length)\n",
    "print(\"Tokenizer vocabulary size:\", tokenizer.vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c2038601-7cee-4e83-ae87-88f030af1581",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def tokenize_text(batch):\n",
    "    return tokenizer(batch[\"text\"], truncation=True, padding=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3338a333-f175-4d1c-9f59-6bbfb5a7dc2b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2a9fd50168204cc790151021ea4ff8d2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/5000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "imdb_tokenized = imdb_dataset.map(tokenize_text, batched=True, batch_size=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d5ce950f-de0b-4db3-898c-3ee553c689e6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "del imdb_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "00bc46c3-f5a5-4fd8-9ebe-cfcb4627ea61",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "imdb_tokenized.set_format(\"torch\", columns=[\"input_ids\", \"attention_mask\", \"label\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f2d54dfa-37f1-47e4-9d2d-25b02257a15c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "754c0560-5802-4b9b-abac-e2441888ba5f",
   "metadata": {},
   "source": [
    "# 3 Set Up DataLoaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "fee3fd8c-0299-4e7f-bd89-8fc72038ecc7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "\n",
    "class IMDBDataset(Dataset):\n",
    "    def __init__(self, dataset_dict, partition_key=\"train\"):\n",
    "        self.partition = dataset_dict[partition_key]\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return self.partition[index]\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.partition.num_rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "190a7421-308d-4361-864a-2686626fa471",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/python310/lib/python3.10/site-packages/torch/utils/data/dataloader.py:561: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  warnings.warn(_create_warning_msg(\n"
     ]
    }
   ],
   "source": [
    "train_dataset = IMDBDataset(imdb_tokenized, partition_key=\"train\")\n",
    "val_dataset = IMDBDataset(imdb_tokenized, partition_key=\"validation\")\n",
    "test_dataset = IMDBDataset(imdb_tokenized, partition_key=\"test\")\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    dataset=train_dataset,\n",
    "    batch_size=12,\n",
    "    shuffle=True, \n",
    "    num_workers=4\n",
    ")\n",
    "\n",
    "val_loader = DataLoader(\n",
    "    dataset=val_dataset,\n",
    "    batch_size=12,\n",
    "    num_workers=4\n",
    ")\n",
    "\n",
    "test_loader = DataLoader(\n",
    "    dataset=test_dataset,\n",
    "    batch_size=12,\n",
    "    num_workers=4\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33416ab1-45cc-44b7-815f-6049ef2b18c6",
   "metadata": {},
   "source": [
    "# 4 Initializing DistilBERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "87cdd67e-8176-4866-978c-c9b5ade955c3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForSequenceClassification\n",
    "\n",
    "num_labels = 2\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    \"distilbert-base-uncased\", num_labels=num_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5a1ae0e8-7ef2-45a3-87fa-c98196ec41c2",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DistilBertForSequenceClassification(\n",
       "  (distilbert): DistilBertModel(\n",
       "    (embeddings): Embeddings(\n",
       "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (transformer): Transformer(\n",
       "      (layer): ModuleList(\n",
       "        (0-5): 6 x TransformerBlock(\n",
       "          (attention): MultiHeadSelfAttention(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (ffn): FFN(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (activation): GELUActivation()\n",
       "          )\n",
       "          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (pre_classifier): Linear(in_features=768, out_features=768, bias=True)\n",
       "  (classifier): Linear(in_features=768, out_features=2, bias=True)\n",
       "  (dropout): Dropout(p=0.2, inplace=False)\n",
       ")"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "26deaa8d-ae33-41fa-981d-db0ab5ea7e80",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of trainable parameters: 66955010\n"
     ]
    }
   ],
   "source": [
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "\n",
    "print(\"Total number of trainable parameters:\", count_parameters(model))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51a169ea-5c53-4c3c-9276-9d77e1138abd",
   "metadata": {},
   "source": [
    "# 5 Finetuning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d650866-b117-4d38-925a-305fd4eac640",
   "metadata": {},
   "source": [
    "**Wrap in LightningModule for Training**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0f357041-bccb-4891-8785-12ea78a3d891",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from local_model_utilities import CustomLightningModule\n",
    "\n",
    "lightning_model = CustomLightningModule(model, num_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6b40aad3-da0a-4bf5-8b47-9321163345ee",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from lightning.pytorch.callbacks import ModelCheckpoint\n",
    "from lightning.pytorch.loggers import CSVLogger\n",
    "\n",
    "\n",
    "callbacks = [\n",
    "    ModelCheckpoint(\n",
    "        save_top_k=1, mode=\"max\", monitor=\"val_acc\"\n",
    "    )  # save top 1 model\n",
    "]\n",
    "logger = CSVLogger(save_dir=\"logs/\", name=\"my-model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b98ef308-fd19-43bd-8e89-8c445ce75009",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using 16bit Automatic Mixed Precision (AMP)\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    }
   ],
   "source": [
    "trainer = L.Trainer(\n",
    "    max_epochs=1,\n",
    "    callbacks=callbacks,\n",
    "    accelerator=\"gpu\",\n",
    "    precision=\"16-mixed\",\n",
    "    devices=1,\n",
    "    logger=logger,\n",
    "    log_every_n_steps=10,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "9084c975-1809-4500-845f-4bfe8410f062",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using a CUDA device ('NVIDIA A100-PCIE-40GB') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "Missing logger folder: logs/my-model\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name     | Type                                | Params\n",
      "-----------------------------------------------------------------\n",
      "0 | model    | DistilBertForSequenceClassification | 67.0 M\n",
      "1 | val_acc  | MulticlassAccuracy                  | 0     \n",
      "2 | test_acc | MulticlassAccuracy                  | 0     \n",
      "-----------------------------------------------------------------\n",
      "67.0 M    Trainable params\n",
      "0         Non-trainable params\n",
      "67.0 M    Total params\n",
      "267.820   Total estimated model params size (MB)\n",
      "SLURM auto-requeueing enabled. Setting signal handlers.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/python310/lib/python3.10/site-packages/torch/utils/data/dataloader.py:561: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  warnings.warn(_create_warning_msg(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b0bf37ced8484d36921851e55447b0bc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=1` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time elapsed 2.14 min\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "start = time.time()\n",
    "\n",
    "trainer.fit(model=lightning_model,\n",
    "            train_dataloaders=train_loader,\n",
    "            val_dataloaders=val_loader)\n",
    "\n",
    "end = time.time()\n",
    "elapsed = end - start\n",
    "print(f\"Time elapsed {elapsed/60:.2f} min\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "8b42c654-89e7-458c-afc2-17664e2871f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Restoring states from the checkpoint path at logs/my-model/version_0/checkpoints/epoch=0-step=2917.ckpt\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "Loaded model weights from the checkpoint at logs/my-model/version_0/checkpoints/epoch=0-step=2917.ckpt\n",
      "SLURM auto-requeueing enabled. Setting signal handlers.\n",
      "/work/LAS/jannesar-lab/.ondemand/dphuong/jupyter-envs/20230427/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:492: Your `test_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "86496e029d584bcbab2e0f2b7058cac2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Testing: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Restoring states from the checkpoint path at logs/my-model/version_0/checkpoints/epoch=0-step=2917.ckpt\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "Loaded model weights from the checkpoint at logs/my-model/version_0/checkpoints/epoch=0-step=2917.ckpt\n",
      "SLURM auto-requeueing enabled. Setting signal handlers.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f5b13edbb2b8462fa26d158547be5931",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Testing: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Restoring states from the checkpoint path at logs/my-model/version_0/checkpoints/epoch=0-step=2917.ckpt\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "Loaded model weights from the checkpoint at logs/my-model/version_0/checkpoints/epoch=0-step=2917.ckpt\n",
      "SLURM auto-requeueing enabled. Setting signal handlers.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "042b141970334f79a0f87f6957308022",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Testing: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_acc = trainer.test(lightning_model, dataloaders=train_loader, ckpt_path=\"best\", verbose=False)\n",
    "val_acc = trainer.test(lightning_model, dataloaders=val_loader, ckpt_path=\"best\", verbose=False)\n",
    "test_acc = trainer.test(lightning_model, dataloaders=test_loader, ckpt_path=\"best\", verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "31b4ef1e-2572-453f-b740-f395536ad3f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train acc: 96.16%\n",
      "Val acc:   92.26%\n",
      "Test acc:  91.75%\n"
     ]
    }
   ],
   "source": [
    "print(f\"Train acc: {train_acc[0]['accuracy']*100:2.2f}%\")\n",
    "print(f\"Val acc:   {val_acc[0]['accuracy']*100:2.2f}%\")\n",
    "print(f\"Test acc:  {test_acc[0]['accuracy']*100:2.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "7cfdf684-3540-4a5c-b121-841b0ecadf89",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import shutil\n",
    "\n",
    "# Cleanup checkpoint files as we don't need them later\n",
    "log_dir = f\"logs/my-model\"\n",
    "if os.path.exists(log_dir):\n",
    "    shutil.rmtree(log_dir)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "6f1bc96d-34f4-4cd6-ba8e-bf11e3ba70bc",
   "metadata": {},
   "source": [
    "Train acc: 96.16%\n",
    "Val acc:   92.26%\n",
    "Test acc:  91.75%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b0e38e7-f9b9-4cc8-b5e6-1aa8e6001805",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
